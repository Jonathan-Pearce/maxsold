#!/usr/bin/env python3
"""
Quick Demo: Before and After Refactoring
Shows what's now possible with the refactored code
"""

print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    REFACTORING COMPLETE! ğŸ‰                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BEFORE REFACTORING âŒ
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Problem: Extraction code was mixed with file I/O
  
  # In 02_extract_auction_details.py
  def main():
      data = fetch_auction_details(auction_id)  # API call
      parsed = extract_from_json(data)          # Parse
      save_to_parquet(parsed, "output.parquet") # Save to file
  
  âŒ Can't reuse fetch/extract without also saving files
  âŒ Can't use for live predictions
  âŒ Code duplicated across multiple scraper files

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
AFTER REFACTORING âœ…
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Solution: Separated extraction logic from I/O operations

  ğŸ“ extractors/
     â”œâ”€â”€ auction_details.py
     â”‚   â”œâ”€â”€ fetch_auction_details()    â† API call only
     â”‚   â””â”€â”€ extract_from_json()        â† Parse only
     â””â”€â”€ ... (other extractors)
  
  ğŸ“ pipelines/
     â””â”€â”€ auction_details_pipeline.py
         â””â”€â”€ run_pipeline()              â† Batch + file I/O
  
  âœ… Extractors can be used standalone
  âœ… Perfect for live ML predictions
  âœ… Zero code duplication

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
USE CASE 1: Data Scraping (Batch Processing)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  # Use CLI scripts (backward compatible)
  $ python 02_extract_auction_details.py --input auctions.parquet
  
  # OR import pipeline directly
  from pipelines import run_auction_details_pipeline
  
  run_auction_details_pipeline(
      auction_ids=["12345", "67890"],
      output_path="data/auction_details.parquet"
  )
  
  â†’ Fetches data from API
  â†’ Parses JSON
  â†’ Saves to parquet file
  â†’ Progress tracking & error handling

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
USE CASE 2: Live ML Predictions (Real-time) ğŸ†•
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  # Import extractors directly
  from extractors import (
      fetch_auction_details,
      extract_auction_from_json,
      fetch_enriched_details,
      extract_enriched_data
  )
  
  # Fetch data on-demand
  auction_json = fetch_auction_details(auction_id="12345")
  auction = extract_auction_from_json(auction_json, "12345")
  
  enriched_json = fetch_enriched_details(item_id="67890")
  enriched = extract_enriched_data(enriched_json, "67890")
  
  # Feed to ML model
  prediction = your_model.predict(auction, enriched)
  
  â†’ Fetches data from API
  â†’ Parses JSON
  â†’ Returns Python dict
  â†’ NO FILES SAVED! âœ¨

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
EXAMPLE: Building a Prediction API
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  from flask import Flask, request, jsonify
  from extractors import fetch_auction_details, extract_auction_from_json
  
  app = Flask(__name__)
  
  @app.route('/predict', methods=['POST'])
  def predict():
      # Get item ID from request
      item_id = request.json['item_id']
      auction_id = request.json['auction_id']
      
      # Fetch data using extractors (no files!)
      auction_json = fetch_auction_details(auction_id)
      auction = extract_auction_from_json(auction_json, auction_id)
      
      # Make prediction
      prediction = ml_model.predict(auction)
      
      # Return result
      return jsonify({
          'item_id': item_id,
          'predicted_price': prediction,
          'confidence': 0.85
      })
  
  # Start API server
  app.run()
  
  âœ… Real-time predictions
  âœ… No temporary files
  âœ… Fast response time
  âœ… Production-ready

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
WHAT EACH EXTRACTOR PROVIDES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  ğŸ“¦ auction_search
     â†’ fetch_sales_search()           : Search auctions by location/date
     â†’ extract_sales_from_json()      : Parse search results
  
  ğŸ“¦ auction_details
     â†’ fetch_auction_details()        : Get auction metadata
     â†’ extract_auction_from_json()    : Parse auction details
  
  ğŸ“¦ item_details
     â†’ fetch_auction_items()          : Get all items in auction
     â†’ extract_items_from_json()      : Parse item list
  
  ğŸ“¦ bid_history
     â†’ fetch_item_bid_history()       : Get bid timeline
     â†’ extract_bids_from_json()       : Parse bid history
  
  ğŸ“¦ item_enriched
     â†’ fetch_enriched_details()       : Get AI-generated data
     â†’ extract_enriched_data()        : Parse enriched details

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
QUICK START
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1ï¸âƒ£  For Live Predictions:
    
    import sys
    sys.path.insert(0, '/workspaces/maxsold/scrapers')
    from extractors import fetch_auction_details, extract_auction_from_json
    
    data = fetch_auction_details("12345")
    parsed = extract_auction_from_json(data, "12345")
    
    # Feed to your model - no files saved!

2ï¸âƒ£  For Batch Scraping:
    
    python 01_extract_auction_search.py --days 180
    python 02_extract_auction_details.py --input auctions.parquet
    # Works exactly as before!

3ï¸âƒ£  See Examples:
    
    examples/live_prediction_example.py
    examples/ml_integration_example.py

4ï¸âƒ£  Read Documentation:
    
    REFACTORING_SUMMARY.md  â† Complete overview
    QUICK_REFERENCE.md      â† Quick start guide
    ARCHITECTURE.md         â† Technical details

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
KEY BENEFITS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  âœ… Code reuse between scraping and live predictions
  âœ… No code duplication
  âœ… Clean separation of concerns
  âœ… Easy to test
  âœ… Backward compatible
  âœ… Production-ready for ML APIs

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
NEXT STEPS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. Read QUICK_REFERENCE.md for usage examples
2. Check examples/ directory for working code
3. Import extractors in your ML pipeline
4. Build your live prediction service!

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ¯ You can now build a live ML model that takes URLs as input and returns
   predictions WITHOUT saving any intermediate files!

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

# Run verification test
print("\nRunning verification tests...\n")
import subprocess
result = subprocess.run(['python', 'test_refactoring.py'], cwd='/workspaces/maxsold/scrapers')
